import os
from pathlib import Path
from typing import Iterable, List, Mapping, Optional, Set, Tuple

import torch
from log import logger
from torch.utils.data import Dataset
import srsly
from transformers import PreTrainedTokenizer

from data.spans import extract_spans, get_mappings, spans_to_labels


# Disable parallel tokenizers
os.environ["TOKENIZERS_PARALLELISM"] = "false"


# Example used for prediction and learning
Example = Tuple[
    torch.Tensor,
    torch.Tensor,
    torch.Tensor,
    Mapping[Tuple[int, int], str],
    torch.Tensor,
]


def _binarize_records(
    records: List[Mapping[str, int | str]], label: str = "OMISSIS"
) -> List[Mapping[int, int | str]]:
    for record in records:
        for entity in record["entities"]:
            entity["label"] = label
    return records


def _discard_tags(
    records: List[Mapping[str, int | str]], tags: Set[str]
) -> List[Mapping[int, int | str]]:
    for record in records:
        filtered_entities = []
        for entity in record["entities"]:
            if entity["label"] not in tags:
                filtered_entities.append(entity)
        record["entities"] = filtered_entities
    return records


class OrdinancesDataset(Dataset):
    def __init__(
        self,
        binarize: bool,
        tokenizer: PreTrainedTokenizer,
        ignore_tags: Set[str] = None,
        label2id: Optional[Mapping[str, int]] = None,
        max_length: int = 512,
        stride: int = 128,
    ) -> None:
        super().__init__()
        self.__binarize = binarize
        self.__ignore_tags = ignore_tags
        self.__tokenizer = tokenizer
        self.__label2id = label2id
        self.__id2label = (
            None
            if label2id is None
            else {idx: label for label, idx in label2id.items()}
        )
        self.__max_length = max_length
        self.__stride = stride
        self.__instances = []
        self.pad_token = tokenizer.special_tokens_map["pad_token"]
        self.pad_token_id = tokenizer.get_vocab()[self.pad_token]
        self.sep_token = tokenizer.special_tokens_map["sep_token"]

    def __compute_token_mask(self, word_ids: List[int]) -> torch.Tensor:
        token_mask = []
        prev_word_idx = None
        for word_idx in word_ids:
            token_mask.append(word_idx is not None and prev_word_idx != word_idx)
            prev_word_idx = word_idx
        return torch.tensor(token_mask, dtype=torch.bool)

    def __encode_record(
        self, text: str, entities: List[Mapping[str, int | str]]
    ) -> Iterable[Example]:
        output = self.__tokenizer(
            text,
            padding=True,
            truncation=True,
            max_length=self.__max_length,
            stride=self.__stride,
            return_tensors="pt",
            return_token_type_ids=False,
            return_attention_mask=True,
            return_overflowing_tokens=True,
            return_offsets_mapping=True,
        )
        # Pops out the vector of overflowing tokens
        sample_mapping = output.pop("overflow_to_sample_mapping")
        # Pops out the offset mapping
        offset_mapping = output.pop("offset_mapping")
        # Encodes the labels generated by the offsets
        for i, sample_idx in enumerate(sample_mapping):
            # Information about the text sequence
            input_ids = output["input_ids"][i]
            attention_mask = output["attention_mask"][i].type(torch.bool)
            # True if the token is the first of its word, False otherwise
            token_mask = self.__compute_token_mask(output.word_ids(i))
            # Charachter offset
            offsets = offset_mapping[i]
            # List of integer labels
            label_ids = spans_to_labels(entities, offsets, self.__label2id)
            # Dictionary of spans
            enc_spans = extract_spans(label_ids, self.__id2label)
            labels = torch.tensor(label_ids)
            yield input_ids, attention_mask, token_mask, enc_spans, labels
        return output

    def read_file(self, filepath: Path) -> None:
        # Loads the raw dataset from disk
        logger.info(f"Reading file {filepath}")
        records = list(srsly.read_jsonl(filepath))
        logger.info(f"Read {len(records)} text records from {filepath}")
        if self.__binarize:
            records = _binarize_records(records)
            logger.info("Dataset binarized")
        elif self.__ignore_tags is not None:
            records = _discard_tags(records, self.__ignore_tags)
            logger.info(f"Tags {self.__ignore_tags} discarded")
        if self.__label2id is None and self.__id2label is None:
            self.__label2id, self.__id2label = get_mappings(records)
            logger.info(
                f"Mappings correctly computed. We have {len(self.__label2id)} tags to predict"
            )
        # Starts the true encoding
        for record in records:
            self.__instances.extend(
                self.__encode_record(record["text"], record["entities"])
            )
        logger.info(f"{len(self.__instances)} instances obtained from {filepath}")

    def get_target_vocab(self) -> Mapping[str, int]:
        if self.__label2id is None:
            raise ValueError("Call `read_file` before accessing this method")
        return self.__label2id

    def get_target_size(self) -> int:
        vocab = self.get_target_vocab()
        return len(vocab.values())

    def __len__(self) -> int:
        length = len(self.__instances)
        if length == 0:
            raise ValueError("Call `read_file` before accessing this method")
        return length

    def __getitem__(self, index: int) -> Example:
        if len(self.__instances) == 0:
            raise ValueError("Call `read_file` before accessing this method")
        return self.__instances[index]

    @classmethod
    def from_file(
        cls,
        filepath: Path,
        binarize: bool,
        tokenizer: PreTrainedTokenizer,
        ignore_tags: Set[str] = None,
        label2id: Optional[Mapping[str, int]] = None,
        max_length: int = 512,
        stride: int = 128,
    ) -> "OrdinancesDataset":
        dataset = OrdinancesDataset(
            binarize, tokenizer, ignore_tags, label2id, max_length, stride
        )
        dataset.read_file(filepath)
        return dataset
